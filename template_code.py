#!/usr/bin/env python
# coding: utf-8

from transformers import AutoTokenizer, BertModel
import sys
import torch
import numpy as np

# Function to compute cosine similarity
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

if __name__ == "__main__":
    model = BertModel.from_pretrained("bert-base-uncased")
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", do_lower_case=True)

    # Read sentences from command line arguments
    sentence1, sentence2 = sys.argv[1:sys.argv.index(",")], sys.argv[sys.argv.index(",")+1:]
    sentence1, sentence2 = " ".join(sentence1), " ".join(sentence2)
    sentences = [sentence1, sentence2]

    # Tokenize each sentence
    sentences_tokenized = [tokenizer.tokenize(s) for s in sentences]
    MAX_LENGTH = max(len(sentences_tokenized[0]), len(sentences_tokenized[1]))

    # Generate IDs of each token and add padding to sentences smaller than given threshold
    ids = [tokenizer.convert_tokens_to_ids(t) for t in sentences_tokenized]
    ids = np.asarray([np.pad(i, (0, MAX_LENGTH-len(i)), mode='constant') for i in ids])

    # Generate the attention masks
    amasks = np.asarray([[float(i > 0) for i in seq] for seq in ids])

    # Get the output from the model
    with torch.no_grad():
        output = model(torch.tensor(ids), attention_mask=torch.tensor(amasks))

    # Extract hidden states (last layer)
    hidden_states = output.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)

    # Pooling vectors (2x768) using max pooling over the hidden states
    pool_vectors = torch.max(hidden_states, dim=1).values  # Max pooling across tokens

    # CLS vectors (2x768) is the vector generated by taking the CLS token
    cls_vectors = hidden_states[:, 0, :]  # First token's hidden state (CLS)

    # Compute the cosine similarity
    cosine_pooling = cosine_similarity(pool_vectors[0].numpy(), pool_vectors[1].numpy())
    cosine_cls = cosine_similarity(cls_vectors[0].numpy(), cls_vectors[1].numpy())

    # Finally print out the values
    print(np.round(cosine_pooling.item(), 2), np.round(cosine_cls.item(), 2))
